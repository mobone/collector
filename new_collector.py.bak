from pandas import read_html, read_csv
import requests
import re
from multiprocessing import Process, Queue
from datetime import datetime
from threading import Thread
import urllib2
from time import sleep, time
import MySQLdb
import warnings
warnings.simplefilter(action = "ignore")
def marketOpen():
    now = datetime.now()

    openTime = now.replace(hour=8,minute=31,second=0,microsecond=0)
    closeTime = now.replace(hour=15,minute=0,second=0,microsecond=0)
    if now>=openTime and now<closeTime:
        return True
    else:
        return False

def get_tickers():

    print "%s getting tickers" % str(datetime.now())
    page = urllib2.urlopen("ftp://ftp.nasdaqtrader.com/SymbolDirectory/options.txt")
    df = read_csv(page,sep="|")

    dfs = df.groupby('Underlying Symbol')
    tickers = []
    for df in dfs:
        if len(df[1])>90:
            tickers.append(df[0])

    print len(tickers)

    f = open('C:/Users/Nick/OneDrive/collector/tickers.csv','w')

    for ticker in tickers:
            f.write(ticker+'\n')
    f.close()
    print "%s getting tickers complete" % str(datetime.now())


def network_worker(ticker_queue):
    s = requests.session()
    sleep(10)
    while marketOpen() or ticker_queue.qsize()>0:
        ticker = ticker_queue.get()
        ticker = ticker.replace('\n','')
        try:
            start = -1
            end = -1
            price = -1
            trycount = 0

            while (start == -1 or end == -1) and trycount<5:
                trycount += 1
                try:
                    html_page = s.get('http://www.marketwatch.com/investing/stock/%s/options?countrycode=US&showAll=True' % ticker.replace('-','.'))
                except Exception as e:
                    print e
                    continue

                if 'No Option Chain found' in html_page.text or 'Symbol Lookup' in html_page.text:
                    break

                html_text = html_page.text.encode('utf-8').strip()

                start = html_text.find('<p class="data bgLast">')
                end = html_text.find('</p>', start)

                try:
                    price = float(html_text[start+23:end])
                except:
                    continue



                start = html_text.find('<p class="data bgLast">')
                end = html_text.find('</table>', start)

                if (start == -1 or end == -1) and trycount>3:
                    print "ERROR " + ticker


            if start != -1 and end != -1 and price != -1:
                #work_queue.put((str(datetime.now()).replace(':','.'), ticker, price, html_page.text[start:end+9]))
                filename = 'c:/to_process/%s %s %s.html' % ((str(datetime.now()).replace(':','.'), ticker, price))
                f = open(filename, 'w')
                f.write(html_page.text[start:end+9])
                f.close()

        except Exception as e:
            print ticker
            print e

            #print html_page.text[start+23:end]


def data_cruncher(work_queue):
    con = MySQLdb.connect(host="localhost", user="root", passwd="cookie", db="collected_options")
    while (marketOpen() or work_queue.qsize()>0):
        current_work_unit = work_queue.get()
        html_text = current_work_unit[3]
        last_price = current_work_unit[2]
        ticker = current_work_unit[1]

        html_text = html_text.replace('<tr class="chainrow acenter understated ', '</table><table class="chainrow acenter understated ')
        html_text = html_text.replace('\n','').replace('\r','')


        tables = html_text.split('<table class="chainrow acenter understated')




        for table in tables:

            table = '<table class="chainrow acenter understated '+str(table)
            date = re.search('(January|February|March|April|May|June|July|August|September|October|November|December)\d{4}',table).group(0)

            df = read_html(table, header=0)[0]
            if 'Change' not in df.columns:
                continue

            cur_price_placement = df[df['Change'].str.contains("Current price")==True]
            if len(cur_price_placement)>0:
                df = df.drop(cur_price_placement.index[0])

            df = df[:-1]



            call_table = df[['Last','Change','Vol','Bid','Ask','Open Int.','Strike']]
            put_table = df[['Last.1','Change.1','Vol.1','Bid.1','Ask.1','Open Int..1']]
            put_table.columns = ['Last','Change','Vol','Bid','Ask','Open Int.']
            put_table['Strike'] = call_table.loc[:,['Strike']]
            call_table['Type'] = 'Call'

            put_table['Type'] = 'Put'

            final_table = call_table.append(put_table)
            final_table['Expiration Date'] = date
            final_table['Last Stock Price'] = last_price
            final_table['Ticker'] = current_work_unit[1]

            final_table['Update Time'] = current_work_unit[0]



            final_table.to_sql(con=con, name='marketwatch_data', if_exists='append', index=False, flavor='mysql')





if __name__ == '__main__':

    get_tickers()


    ticker_queue = Queue()


    # wait for market open

    while marketOpen() == False:
        sleep(1)

    for i in range(35):
        t = Thread(target=network_worker, args = (ticker_queue,)).start()

    #for i in range(3):
        #p = Process(target = data_cruncher, args = (work_queue,)).start()

    while marketOpen():
        f = open('C:/Users/Nick/OneDrive/collector/tickers.csv', 'r')

        for i in f.readlines():
            ticker_queue.put(i)
        f.close()
        #print "New queue length: "+str(ticker_queue.qsize())

        now = time()
        while ticker_queue.qsize()>0:
            sleep(1)
            #print str(work_queue.qsize()) + ' ',

        print '\n%s Queue completed in %i seconds' % (str(datetime.now()), time()-now)

        while time()-now < 900:
            #print str(work_queue.qsize()) + ' ',
            sleep(1)

    while ticker_queue.qsize()>0:
        sleep(1)

    del(ticker_queue)
